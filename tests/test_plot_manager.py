# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest

from .common import test_result_collector as trc
from .common.test_utils import convert_to_bytes, ROOT_DIR
from .mocks.mock_config import MockConfig

from model_analyzer.cli.cli import CLI
from model_analyzer.config.input.config_command_analyze \
    import ConfigCommandAnalyze

from model_analyzer.plots.plot_manager import PlotManager
from model_analyzer.result.result_manager import ResultManager
from model_analyzer.state.analyzer_state_manager import AnalyzerStateManager

from filecmp import cmp
from shutil import rmtree
from unittest.mock import MagicMock, patch

import pickle


class TestPlotManager(trc.TestResultCollector):

    def setUp(self):
        self._create_single_model_result_manager()
        self._create_multi_model_result_manager()

    def tearDown(self):
        patch.stopall()

    def test_single_model_summary_plots_against_golden(self):
        """
        Match the summary plots against the golden versions in
        tests/common/single-model-ckpt
        """
        plot_manager = PlotManager(
            config=self._single_model_config,
            result_manager=self._single_model_result_manager)

        plot_manager.create_summary_plots()

        # Uncomment these lines to create a new output dump to compare against
        # file = open(f"{ROOT_DIR}/single-model-ckpt/plot_manager.pkl", "wb")
        # pickle.dump(plot_manager._simple_plots, file)
        # file.close()

        file = open(f"{ROOT_DIR}/single-model-ckpt/plot_manager.pkl", "rb")
        golden_plot_manager_simple_plots = pickle.load(file)
        file.close()

        golden = golden_plot_manager_simple_plots['add_sub'][
            'throughput_v_latency']
        plot = plot_manager._simple_plots['add_sub']['throughput_v_latency']
        self._compare_plot_data(golden, plot)

        golden = golden_plot_manager_simple_plots['add_sub'][
            'gpu_mem_v_latency']
        plot = plot_manager._simple_plots['add_sub']['gpu_mem_v_latency']
        self._compare_plot_data(golden, plot)

    def test_multi_model_summary_plots_against_golden(self):
        """
        Match the summary plots against the golden versions in
        tests/common/multi-model-ckpt
        """
        plot_manager = PlotManager(
            config=self._multi_model_config,
            result_manager=self._multi_model_result_manager)

        plot_manager.create_summary_plots()

        # Uncomment these lines to create a new output dump to compare against
        # file = open(f"{ROOT_DIR}/multi-model-ckpt/plot_manager.pkl", "wb")
        # pickle.dump(plot_manager._simple_plots, file)
        # file.close()

        file = open(f"{ROOT_DIR}/multi-model-ckpt/plot_manager.pkl", "rb")
        golden_plot_manager_simple_plots = pickle.load(file)
        file.close()

        golden = golden_plot_manager_simple_plots[
            'resnet50_libtorch,vgg19_libtorch']['throughput_v_latency']
        plot = plot_manager._simple_plots['resnet50_libtorch,vgg19_libtorch'][
            'throughput_v_latency']
        self._compare_plot_data(golden, plot)

        golden = golden_plot_manager_simple_plots[
            'resnet50_libtorch,vgg19_libtorch']['gpu_mem_v_latency']
        plot = plot_manager._simple_plots['resnet50_libtorch,vgg19_libtorch'][
            'gpu_mem_v_latency']
        self._compare_plot_data(golden, plot)

    def _compare_plot_data(self, golden, plot):
        self.assertEqual(golden._name, plot._name)
        self.assertEqual(golden._title, plot._title)
        self.assertEqual(golden._x_axis, plot._x_axis)
        self.assertEqual(golden._y_axis, plot._y_axis)
        self.assertEqual(golden._monotonic, plot._monotonic)
        self.assertEqual(golden._data, plot._data)

        # These are generated by pyplot and will vary from run to run
        #self.assertEqual(golden._fig, plot._fig)
        #self.assertEqual(golden._ax, plot._ax)

    def _evaluate_config(self, args, yaml_content):
        mock_config = MockConfig(args, yaml_content)
        mock_config.start()
        config = ConfigCommandAnalyze()
        cli = CLI()
        cli.add_subcommand(
            cmd='analyze',
            help=
            'Collect and sort profiling results and generate data and summaries.',
            config=config)
        cli.parse()
        mock_config.stop()
        return config

    def _create_single_model_result_manager(self):
        args = [
            'model-analyzer', 'analyze', '-f', 'config.yml',
            '--checkpoint-directory', f'{ROOT_DIR}/single-model-ckpt/',
            '--export-path', f'{ROOT_DIR}/single-model-ckpt/'
        ]
        yaml_content = convert_to_bytes("""
            analysis_models: add_sub
        """)
        config = self._evaluate_config(args, yaml_content)
        state_manager = AnalyzerStateManager(config=config, server=None)
        state_manager.load_checkpoint(checkpoint_required=True)

        self._single_model_result_manager = ResultManager(
            config=config, state_manager=state_manager)

        self._single_model_config = config

        self._single_model_result_manager.create_tables()
        self._single_model_result_manager.compile_and_sort_results()

    def _create_multi_model_result_manager(self):
        args = [
            'model-analyzer', 'analyze', '-f', 'config.yml',
            '--checkpoint-directory', f'{ROOT_DIR}/multi-model-ckpt/',
            '--export-path', f'{ROOT_DIR}/multi-model-ckpt/'
        ]
        yaml_content = convert_to_bytes("""
            analysis_models: resnet50_libtorch,vgg19_libtorch
        """)
        config = self._evaluate_config(args, yaml_content)
        state_manager = AnalyzerStateManager(config=config, server=None)
        state_manager.load_checkpoint(checkpoint_required=True)

        self._multi_model_result_manager = ResultManager(
            config=config, state_manager=state_manager)

        self._multi_model_config = config

        self._multi_model_result_manager.create_tables()
        self._multi_model_result_manager.compile_and_sort_results()


if __name__ == "__main__":
    unittest.main()
