# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .config_generator_interface import ConfigGeneratorInterface
from .model_config_generator import ModelConfigGenerator
from .perf_analyzer_config_generator import PerfAnalyzerConfigGenerator

from model_analyzer.config.run.run_config import RunConfig


class RunConfigGenerator(ConfigGeneratorInterface):
    """
    Generates all RunConfigs to execute for the given model
    """

    def __init__(self, config, models, client):
        """
        Parameters
        ----------
        config: ModelAnalyzerConfig
        
        models: List of ConfigModelProfileSpec
            The list of models to generate RunConfigs for
            
        client: TritonClient
        """
        self._config = config
        self._models = models

        # MM-PHASE 0: Assuming that all models are identical, so using first model's name/flag/parameters
        self._model_name = models[0].model_name()
        self._model_pa_flags = models[0].perf_analyzer_flags()
        self._model_parameters = models[0].parameters()

        self._curr_mcgs = [
            ModelConfigGenerator(config, model, client) for model in models
        ]

        self._curr_model_configs = [
            mcg.next_config() for mcg in self._curr_mcgs
        ]

        # MM-PHASE 0: Assuming that all models are identical, so using first model's name
        self._curr_pacg = PerfAnalyzerConfigGenerator(
            self._config, self._curr_model_configs[0].get_field("name"),
            self._model_pa_flags, self._model_parameters)

    def is_done(self):
        """ Returns true if this generator is done generating configs """
        # MM-PHASE 0: Assuming that all models are identical,
        # so we only need to check for when the first model completes
        return self._curr_mcgs[0].is_done() and self._curr_pacg.is_done()

    def next_config(self):
        """
        Returns
        -------
        RunConfig
            The next RunConfig generated by this class
        """
        if self._curr_pacg.is_done():

            self._curr_model_configs = [
                mcg.next_config() for mcg in self._curr_mcgs
            ]

            # MM-PHASE 0: Assuming that all models are identical, so using first model's name
            self._curr_pacg = PerfAnalyzerConfigGenerator(
                self._config, self._curr_model_configs[0].get_field("name"),
                self._model_pa_flags, self._model_parameters)

        perf_analyzer_config = self._curr_pacg.next_config()
        run_config = self._generate_run_config(self._curr_model_configs,
                                               perf_analyzer_config)

        return run_config

    def set_last_results(self, measurements):
        """ 
        Given the results from the last RunConfig, make decisions 
        about future configurations to generate

        Parameters
        ----------
        measurements: List of Measurements from the last run(s)
        """
        self._curr_pacg.set_last_results(measurements)

    def _generate_run_config(self, model_configs, perf_analyzer_config):
        # MM-PHASE 0: Assuming that all models are identical, so using first model's server env
        run_config = RunConfig(self._model_name, model_configs,
                               perf_analyzer_config,
                               self._models[0].triton_server_environment())
        return run_config
